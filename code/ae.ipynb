{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a91609a7-e84f-4c21-a173-692a0551c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3b36f9-4cc6-431b-a232-8b52c87fe8fe",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b01e4d9-37d1-4ef7-9f51-c2e11f07bcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import numpy as np\n",
    "import nibabel as ni\n",
    "import os, shutil\n",
    "import time\n",
    "import random\n",
    "import pandas as pd \n",
    "from scipy.ndimage import zoom\n",
    "import matplotlib.pyplot as plt\n",
    "import loss\n",
    "# from model_VAE import VAE\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d619ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRI_dataloader(Dataset):\n",
    "    def __init__(self, path, img_size=(137, 128, 128)):\n",
    "        self.path = path\n",
    "        self.filenames = [i for i in os.listdir(path) if i.endswith(\".nii\")]\n",
    "        #self.upsample = torch.nn.Upsample(size=(137, 128, 128), mode='trilinear', align_corners=True)\n",
    "        self.img_size = img_size\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image = ni.load(os.path.join(self.path, self.filenames[index]))\n",
    "        image = np.array(image.dataobj)\n",
    "        image = np.moveaxis(image, [0, 1], [1, 0])\n",
    "        img_scale = tuple([self.img_size[i]/image.shape[i] for i in range(len(image.shape))])\n",
    "        image = zoom(image, img_scale)\n",
    "#         image = image / 255.\n",
    "        image = torch.from_numpy(image)\n",
    "        image = image.unsqueeze(0)\n",
    "        #image = torch.permute(image, (1, 0, 2)).unsqueeze(0)\n",
    "        return image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d25fe18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                     | 0/171 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "# ckpt_path_VAE = './checkpoint/best_model.pt' \n",
    "# ckpt_path_AE = './checkpoint/best_model_AE.pt'\n",
    "\n",
    "ckpt_path_AE = './checkpoint/best_model_AE_l1_loss.pt'\n",
    "ckpt_path_VAE = './checkpoint/best_model_vae_l1.pt'\n",
    "\n",
    "model_AE = torch.load(ckpt_path_AE).to(device)\n",
    "# model_VAE = torch.load(ckpt_path_VAE).to(device)\n",
    "\n",
    "\n",
    "# ckpt_path = path2save = \"./model_VAE.pt\"\n",
    "# ckpt_path = ckpt_path.format(52)\n",
    "imagePath = 'dataset/images'\n",
    "mri_loader = MRI_dataloader(imagePath)\n",
    "batch_size = 1\n",
    "dataloader = DataLoader(mri_loader, batch_size=batch_size, shuffle=True)\n",
    "# model = VAE().to(device)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "labels = pd.read_csv('dataset/data.csv')\n",
    "\n",
    "model_AE.eval()\n",
    "for batch in tqdm(dataloader):\n",
    "    batch = batch.to(device)\n",
    "    # print(model_AE(batch))\n",
    "    z, y = model_AE(batch)\n",
    "    print(y.shape)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d9d27c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "140db27c-b756-4735-b042-87db80a0a613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = model_AE(batch)\n",
    "# print(len(out))\n",
    "# out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fde9ef33-39b2-4e6e-a861-5b2639667bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5da6f62b-f791-4338-a352-329e615454e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 512\n",
    "epsilon = torch.normal(size=(1, latent_dim), mean=0, std=1.0, device=device)\n",
    "# z = z_mean + z_log_sigma.exp()*epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3680767-c956-48b5-89e5-176c6084ff06",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d405b071-4c3b-4c02-a719-74926de47fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import zoom\n",
    "import nibabel as ni\n",
    "\n",
    "# def preprocess_image(path, filename, img_size=(137, 128, 128)):\n",
    "#     image = ni.load(os.path.join(path, filename))\n",
    "#     image = np.array(image.dataobj)\n",
    "#     image = np.moveaxis(image, [0, 1], [1, 0])\n",
    "#     img_scale = tuple([img_size[i]/image.shape[i] for i in range(len(image.shape))])\n",
    "#     image = zoom(image, img_scale)\n",
    "#     # image = image / 255.\n",
    "#     image = torch.from_numpy(image)\n",
    "#     image = image.unsqueeze(0)  # Add channel dimension\n",
    "#     # No need to add batch dimension here; will adjust when calling the model\n",
    "#     return image\n",
    "\n",
    "def preprocess(input_path):\n",
    "    \n",
    "    img_size=(137, 128, 128)\n",
    "    image = ni.load(input_path)\n",
    "    image = np.array(image.dataobj)\n",
    "    image = np.moveaxis(image, [0, 1], [1, 0])\n",
    "    img_scale = tuple([img_size[i]/image.shape[i] for i in range(len(image.shape))])\n",
    "    image = zoom(image, img_scale)\n",
    "    image = torch.from_numpy(image)\n",
    "    image = image.unsqueeze(0).unsqueeze(0)\n",
    "    image = image.to(device)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a970977-821f-4c6a-a38c-b6a958f7cfcf",
   "metadata": {},
   "source": [
    "# Getting Latents from VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05bf6290-4b37-4d15-96d5-7b30c89af554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted for processing multiple files\n",
    "filenames = [i for i in os.listdir(imagePath) if i.endswith(\".nii\")]\n",
    "filenames = sorted(filenames)\n",
    "model_AE.eval()  # Make sure the model is in evaluation mode\n",
    "latents = []\n",
    "PT_500 = []\n",
    "PT_4000 = []\n",
    "fileName = None\n",
    "\n",
    "\n",
    "for filename in filenames:\n",
    "    # img = preprocess_image(path, filename)\n",
    "    fullPath = os.path.join(imagePath, filename)\n",
    "    img = preprocess(fullPath)\n",
    "    \n",
    "    # Forward pass through the model\n",
    "    with torch.no_grad():\n",
    "        z, latent = model_AE(img)\n",
    "        # latent = model_AE(img)\n",
    "        #z.append(z_log_sigma)\n",
    "    \n",
    "    # Directly use z_mean as the latent representation\n",
    "    # latent = z_mean.detach().cpu().numpy().flatten()  # Flatten or adjust according to your needs\n",
    "    # Extract labels using filename manipulation\n",
    "    filename_ = filename.split('_')[0]\n",
    "    fl = filename_[:5] + '_' + filename_[5:]\n",
    "    pt500_val = labels[labels['ID'] == fl]['PT500'].iloc[0] if not labels[labels['ID'] == fl].empty else None\n",
    "    pt4000_val = labels[labels['ID'] == fl]['PT4000'].iloc[0] if not labels[labels['ID'] == fl].empty else None\n",
    "    \n",
    "    if pt500_val is not None and pt4000_val is not None:\n",
    "        PT_500.append(pt500_val)\n",
    "        PT_4000.append(pt4000_val)\n",
    "        latents.append(latent.detach().cpu().numpy())\n",
    "        \n",
    "\n",
    "# Remove the break to process all files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e86b5842-3b37-4541-a098-686a9a356802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['smwp10001_T1.nii',\n",
       " 'smwp10002_T1.nii',\n",
       " 'smwp10003_T1.nii',\n",
       " 'smwp10004_T1.nii',\n",
       " 'smwp10005_T1.nii',\n",
       " 'smwp10006_T1.nii',\n",
       " 'smwp10007_T1.nii',\n",
       " 'smwp10008_T1.nii',\n",
       " 'smwp10009_T1.nii',\n",
       " 'smwp10010_T1.nii',\n",
       " 'smwp10011_T1.nii',\n",
       " 'smwp10012_T1.nii',\n",
       " 'smwp10013_T1.nii',\n",
       " 'smwp10014_T1.nii',\n",
       " 'smwp10015_T1.nii',\n",
       " 'smwp10016_T1.nii',\n",
       " 'smwp10017_T1.nii',\n",
       " 'smwp10018_T1.nii',\n",
       " 'smwp10019_T1.nii',\n",
       " 'smwp10020_T1.nii',\n",
       " 'smwp10021_T1.nii',\n",
       " 'smwp10022_T1.nii',\n",
       " 'smwp10023_T1.nii',\n",
       " 'smwp10024_T1.nii',\n",
       " 'smwp10025_T1.nii',\n",
       " 'smwp10026_T1.nii',\n",
       " 'smwp10027_T1.nii',\n",
       " 'smwp10028_T1.nii',\n",
       " 'smwp10029_T1.nii',\n",
       " 'smwp10030_T1.nii',\n",
       " 'smwp10031_T1.nii',\n",
       " 'smwp10032_T1.nii',\n",
       " 'smwp10033_T1.nii',\n",
       " 'smwp10034_T1.nii',\n",
       " 'smwp10035_T1.nii',\n",
       " 'smwp10036_T1.nii',\n",
       " 'smwp10037_T1.nii',\n",
       " 'smwp10038_T1.nii',\n",
       " 'smwp10039_T1.nii',\n",
       " 'smwp10040_T1.nii',\n",
       " 'smwp10041_T1.nii',\n",
       " 'smwp10042_T1.nii',\n",
       " 'smwp10043_T1.nii',\n",
       " 'smwp10044_T1.nii',\n",
       " 'smwp10045_T1.nii',\n",
       " 'smwp10046_T1.nii',\n",
       " 'smwp10047_T1.nii',\n",
       " 'smwp10048_T1.nii',\n",
       " 'smwp10049_T1.nii',\n",
       " 'smwp10050_T1.nii',\n",
       " 'smwp10051_T1.nii',\n",
       " 'smwp10052_T1.nii',\n",
       " 'smwp10053_T1.nii',\n",
       " 'smwp10054_T1.nii',\n",
       " 'smwp10055_T1.nii',\n",
       " 'smwp10056_T1.nii',\n",
       " 'smwp10057_T1.nii',\n",
       " 'smwp10058_T1.nii',\n",
       " 'smwp10059_T1.nii',\n",
       " 'smwp10060_T1.nii',\n",
       " 'smwp10061_T1.nii',\n",
       " 'smwp10062_T1.nii',\n",
       " 'smwp10063_T1.nii',\n",
       " 'smwp10064_T1.nii',\n",
       " 'smwp10065_T1.nii',\n",
       " 'smwp10066_T1.nii',\n",
       " 'smwp10067_T1.nii',\n",
       " 'smwp10068_T1.nii',\n",
       " 'smwp10069_T1.nii',\n",
       " 'smwp10070_T1.nii',\n",
       " 'smwp10071_T1.nii',\n",
       " 'smwp10072_T1.nii',\n",
       " 'smwp10073_T1.nii',\n",
       " 'smwp10074_T1.nii',\n",
       " 'smwp10075_T1.nii',\n",
       " 'smwp10076_T1.nii',\n",
       " 'smwp10077_T1.nii',\n",
       " 'smwp10078_T1.nii',\n",
       " 'smwp10079_T1.nii',\n",
       " 'smwp10080_T1.nii',\n",
       " 'smwp10081_T1.nii',\n",
       " 'smwp10082_T1.nii',\n",
       " 'smwp10083_T1.nii',\n",
       " 'smwp10084_T1.nii',\n",
       " 'smwp10085_T1.nii',\n",
       " 'smwp10086_T1.nii',\n",
       " 'smwp10087_T1.nii',\n",
       " 'smwp10088_T1.nii',\n",
       " 'smwp10089_T1.nii',\n",
       " 'smwp10090_T1.nii',\n",
       " 'smwp10091_T1.nii',\n",
       " 'smwp10092_T1.nii',\n",
       " 'smwp10093_T1.nii',\n",
       " 'smwp10094_T1.nii',\n",
       " 'smwp10095_T1.nii',\n",
       " 'smwp10096_T1.nii',\n",
       " 'smwp10097_T1.nii',\n",
       " 'smwp10098_T1.nii',\n",
       " 'smwp10099_T1.nii',\n",
       " 'smwp10100_T1.nii',\n",
       " 'smwp10101_T1.nii',\n",
       " 'smwp10102_T1.nii',\n",
       " 'smwp10103_T1.nii',\n",
       " 'smwp10104_T1.nii',\n",
       " 'smwp10105_T1.nii',\n",
       " 'smwp10106_T1.nii',\n",
       " 'smwp10107_T1.nii',\n",
       " 'smwp10108_T1.nii',\n",
       " 'smwp10109_T1.nii',\n",
       " 'smwp10110_T1.nii',\n",
       " 'smwp10111_T1.nii',\n",
       " 'smwp10112_T1.nii',\n",
       " 'smwp10113_T1.nii',\n",
       " 'smwp10114_T1.nii',\n",
       " 'smwp10115_T1.nii',\n",
       " 'smwp10116_T1.nii',\n",
       " 'smwp10117_T1.nii',\n",
       " 'smwp10118_T1.nii',\n",
       " 'smwp10119_T1.nii',\n",
       " 'smwp10120_T1.nii',\n",
       " 'smwp10121_T1.nii',\n",
       " 'smwp10122_T1.nii',\n",
       " 'smwp10123_T1.nii',\n",
       " 'smwp10124_T1.nii',\n",
       " 'smwp10125_T1.nii',\n",
       " 'smwp10126_T1.nii',\n",
       " 'smwp10127_T1.nii',\n",
       " 'smwp10128_T1.nii',\n",
       " 'smwp10129_T1.nii',\n",
       " 'smwp10130_T1.nii',\n",
       " 'smwp10131_T1.nii',\n",
       " 'smwp10132_T1.nii',\n",
       " 'smwp10133_T1.nii',\n",
       " 'smwp10134_T1.nii',\n",
       " 'smwp10135_T1.nii',\n",
       " 'smwp10136_T1.nii',\n",
       " 'smwp10137_T1.nii',\n",
       " 'smwp10138_T1.nii',\n",
       " 'smwp10139_T1.nii',\n",
       " 'smwp10140_T1.nii',\n",
       " 'smwp10141_T1.nii',\n",
       " 'smwp10142_T1.nii',\n",
       " 'smwp10143_T1.nii',\n",
       " 'smwp10144_T1.nii',\n",
       " 'smwp10145_T1.nii',\n",
       " 'smwp10146_T1.nii',\n",
       " 'smwp10147_T1.nii',\n",
       " 'smwp10148_T1.nii',\n",
       " 'smwp10149_T1.nii',\n",
       " 'smwp10150_T1.nii',\n",
       " 'smwp10151_T1.nii',\n",
       " 'smwp10152_T1.nii',\n",
       " 'smwp10153_T1.nii',\n",
       " 'smwp10154_T1.nii',\n",
       " 'smwp10155_T1.nii',\n",
       " 'smwp10156_T1.nii',\n",
       " 'smwp10157_T1.nii',\n",
       " 'smwp10158_T1.nii',\n",
       " 'smwp10159_T1.nii',\n",
       " 'smwp10160_T1.nii',\n",
       " 'smwp10161_T1.nii',\n",
       " 'smwp10162_T1.nii',\n",
       " 'smwp10163_T1.nii',\n",
       " 'smwp10164_T1.nii',\n",
       " 'smwp10165_T1.nii',\n",
       " 'smwp10166_T1.nii',\n",
       " 'smwp10167_T1.nii',\n",
       " 'smwp10168_T1.nii',\n",
       " 'smwp10169_T1.nii',\n",
       " 'smwp10170_T1.nii',\n",
       " 'smwp10171_T1.nii']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5d32a33-1f69-45f8-857c-ba858e2e4cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False,  True, False,  True,  True, False,  True,  True, False,\n",
       "        False, False, False,  True, False, False,  True,  True, False,\n",
       "        False,  True,  True, False,  True,  True,  True, False, False,\n",
       "         True, False,  True,  True,  True, False,  True,  True,  True,\n",
       "         True,  True,  True, False,  True, False, False,  True,  True,\n",
       "         True,  True, False,  True,  True,  True, False,  True,  True,\n",
       "         True,  True, False, False,  True, False,  True,  True, False,\n",
       "         True,  True,  True,  True, False,  True, False,  True,  True,\n",
       "        False, False,  True,  True,  True, False,  True,  True,  True,\n",
       "        False, False, False, False, False,  True,  True,  True,  True,\n",
       "        False,  True,  True,  True, False,  True, False,  True,  True,\n",
       "        False, False,  True, False,  True, False,  True, False, False,\n",
       "         True,  True, False, False, False, False,  True,  True,  True,\n",
       "        False,  True, False,  True,  True, False,  True,  True,  True,\n",
       "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
       "         True, False,  True, False,  True, False,  True, False,  True,\n",
       "         True, False,  True,  True, False,  True, False,  True,  True,\n",
       "        False,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "         True, False,  True, False,  True, False, False,  True, False,\n",
       "        False,  True,  True,  True, False,  True,  True,  True, False,\n",
       "        False,  True, False,  True, False,  True,  True,  True,  True,\n",
       "         True,  True,  True, False,  True,  True, False, False, False,\n",
       "        False,  True,  True,  True, False,  True, False,  True,  True,\n",
       "        False, False,  True,  True,  True,  True,  True, False, False,\n",
       "        False, False, False,  True,  True,  True,  True, False,  True,\n",
       "         True, False,  True,  True,  True, False,  True, False,  True,\n",
       "        False,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "         True,  True, False,  True,  True,  True, False,  True,  True,\n",
       "        False,  True, False,  True,  True,  True, False,  True,  True,\n",
       "         True, False,  True,  True, False, False,  True,  True,  True,\n",
       "        False, False, False,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True, False,  True,  True, False, False,  True,  True,\n",
       "        False, False,  True,  True, False,  True, False,  True,  True,\n",
       "        False,  True,  True, False,  True, False,  True, False,  True,\n",
       "        False,  True, False,  True,  True,  True,  True,  True, False,\n",
       "        False, False,  True, False,  True, False,  True,  True,  True,\n",
       "         True,  True,  True,  True, False, False,  True,  True, False,\n",
       "        False,  True, False, False,  True,  True,  True,  True,  True,\n",
       "         True, False,  True,  True, False,  True,  True,  True,  True,\n",
       "         True, False, False,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        False, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        False,  True, False,  True,  True,  True, False, False, False,\n",
       "        False,  True, False,  True,  True,  True, False,  True,  True,\n",
       "        False, False, False, False, False,  True, False,  True, False,\n",
       "         True, False,  True,  True,  True, False,  True, False,  True,\n",
       "        False,  True, False,  True,  True,  True, False,  True, False,\n",
       "         True, False, False, False,  True,  True,  True, False,  True,\n",
       "         True,  True,  True, False,  True,  True,  True, False,  True,\n",
       "         True,  True,  True, False,  True, False, False,  True, False,\n",
       "        False,  True,  True,  True, False, False,  True,  True,  True,\n",
       "         True,  True, False, False,  True, False,  True, False,  True,\n",
       "        False, False, False,  True,  True,  True, False, False,  True,\n",
       "        False, False,  True,  True, False, False,  True, False, False,\n",
       "         True,  True, False, False,  True,  True,  True,  True, False,\n",
       "         True, False,  True,  True,  True, False,  True,  True, False,\n",
       "        False,  True,  True,  True, False, False, False,  True]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latents[0] == latents[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadee4ab-6f90-4e75-a2f1-3a6bd9a6a277",
   "metadata": {},
   "source": [
    "# TEST TRAIN SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea3f3310-d24e-4edc-9284-b1de759e5cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filenames = [\n",
    "'smwp10067_T1.nii'\n",
    ",'smwp10051_T1.nii'\n",
    ",'smwp10071_T1.nii'\n",
    ",'smwp10074_T1.nii'\n",
    ",'smwp10123_T1.nii'\n",
    ",'smwp10066_T1.nii'\n",
    ",'smwp10058_T1.nii'\n",
    ",'smwp10148_T1.nii'\n",
    ",'smwp10155_T1.nii'\n",
    ",'smwp10076_T1.nii'\n",
    ",'smwp10002_T1.nii'\n",
    ",'smwp10124_T1.nii'\n",
    ",'smwp10108_T1.nii'\n",
    ",'smwp10151_T1.nii'\n",
    ",'smwp10042_T1.nii'\n",
    ",'smwp10117_T1.nii'\n",
    ",'smwp10046_T1.nii'\n",
    ",'smwp10092_T1.nii'\n",
    ",'smwp10061_T1.nii'\n",
    ",'smwp10088_T1.nii'\n",
    ",'smwp10162_T1.nii'\n",
    ",'smwp10165_T1.nii'\n",
    ",'smwp10040_T1.nii'\n",
    ",'smwp10113_T1.nii'\n",
    ",'smwp10028_T1.nii'\n",
    ",'smwp10116_T1.nii'\n",
    ",'smwp10024_T1.nii'\n",
    ",'smwp10118_T1.nii'\n",
    ",'smwp10095_T1.nii'\n",
    ",'smwp10016_T1.nii'\n",
    ",'smwp10039_T1.nii'\n",
    ",'smwp10049_T1.nii'\n",
    ",'smwp10126_T1.nii'\n",
    ",'smwp10159_T1.nii'\n",
    ",'smwp10070_T1.nii'\n",
    ",'smwp10130_T1.nii'\n",
    ",'smwp10004_T1.nii'\n",
    ",'smwp10132_T1.nii'\n",
    ",'smwp10008_T1.nii'\n",
    ",'smwp10005_T1.nii'\n",
    ",'smwp10093_T1.nii'\n",
    ",'smwp10035_T1.nii'\n",
    ",'smwp10169_T1.nii'\n",
    ",'smwp10122_T1.nii'\n",
    ",'smwp10125_T1.nii'\n",
    ",'smwp10021_T1.nii'\n",
    ",'smwp10153_T1.nii'\n",
    ",'smwp10033_T1.nii'\n",
    ",'smwp10012_T1.nii'\n",
    ",'smwp10025_T1.nii'\n",
    ",'smwp10131_T1.nii'\n",
    ",'smwp10014_T1.nii'\n",
    ",'smwp10141_T1.nii'\n",
    ",'smwp10007_T1.nii'\n",
    ",'smwp10069_T1.nii'\n",
    ",'smwp10087_T1.nii'\n",
    ",'smwp10102_T1.nii'\n",
    ",'smwp10086_T1.nii'\n",
    ",'smwp10072_T1.nii'\n",
    ",'smwp10083_T1.nii'\n",
    ",'smwp10101_T1.nii'\n",
    ",'smwp10163_T1.nii'\n",
    ",'smwp10171_T1.nii'\n",
    ",'smwp10010_T1.nii'\n",
    ",'smwp10115_T1.nii'\n",
    ",'smwp10112_T1.nii'\n",
    ",'smwp10157_T1.nii'\n",
    ",'smwp10062_T1.nii'\n",
    ",'smwp10094_T1.nii'\n",
    ",'smwp10136_T1.nii'\n",
    ",'smwp10097_T1.nii'\n",
    ",'smwp10037_T1.nii'\n",
    ",'smwp10146_T1.nii'\n",
    ",'smwp10011_T1.nii'\n",
    ",'smwp10129_T1.nii'\n",
    ",'smwp10045_T1.nii'\n",
    ",'smwp10104_T1.nii'\n",
    ",'smwp10057_T1.nii'\n",
    ",'smwp10152_T1.nii'\n",
    ",'smwp10001_T1.nii'\n",
    ",'smwp10023_T1.nii'\n",
    ",'smwp10013_T1.nii'\n",
    ",'smwp10009_T1.nii'\n",
    ",'smwp10075_T1.nii'\n",
    ",'smwp10127_T1.nii'\n",
    ",'smwp10144_T1.nii'\n",
    ",'smwp10017_T1.nii'\n",
    ",'smwp10022_T1.nii'\n",
    ",'smwp10114_T1.nii'\n",
    ",'smwp10119_T1.nii'\n",
    ",'smwp10044_T1.nii'\n",
    ",'smwp10156_T1.nii'\n",
    ",'smwp10050_T1.nii'\n",
    ",'smwp10142_T1.nii'\n",
    ",'smwp10019_T1.nii'\n",
    ",'smwp10081_T1.nii'\n",
    ",'smwp10077_T1.nii'\n",
    ",'smwp10064_T1.nii'\n",
    ",'smwp10048_T1.nii'\n",
    ",'smwp10026_T1.nii'\n",
    ",'smwp10018_T1.nii'\n",
    ",'smwp10170_T1.nii'\n",
    ",'smwp10060_T1.nii'\n",
    ",'smwp10031_T1.nii'\n",
    ",'smwp10030_T1.nii'\n",
    ",'smwp10138_T1.nii'\n",
    ",'smwp10168_T1.nii'\n",
    ",'smwp10134_T1.nii'\n",
    ",'smwp10043_T1.nii'\n",
    ",'smwp10145_T1.nii'\n",
    ",'smwp10079_T1.nii'\n",
    ",'smwp10054_T1.nii'\n",
    ",'smwp10161_T1.nii'\n",
    ",'smwp10103_T1.nii'\n",
    ",'smwp10073_T1.nii'\n",
    ",'smwp10099_T1.nii'\n",
    ",'smwp10041_T1.nii'\n",
    ",'smwp10065_T1.nii'\n",
    ",'smwp10109_T1.nii'\n",
    "]\n",
    "\n",
    "val_filenames = [\n",
    "'smwp10034_T1.nii'\n",
    ",'smwp10111_T1.nii'\n",
    ",'smwp10003_T1.nii'\n",
    ",'smwp10098_T1.nii'\n",
    ",'smwp10029_T1.nii'\n",
    ",'smwp10096_T1.nii'\n",
    ",'smwp10006_T1.nii'\n",
    ",'smwp10027_T1.nii'\n",
    ",'smwp10059_T1.nii'\n",
    ",'smwp10166_T1.nii'\n",
    ",'smwp10147_T1.nii'\n",
    ",'smwp10082_T1.nii'\n",
    ",'smwp10143_T1.nii'\n",
    ",'smwp10110_T1.nii'\n",
    ",'smwp10139_T1.nii'\n",
    ",'smwp10167_T1.nii'\n",
    ",'smwp10089_T1.nii'\n",
    ",'smwp10154_T1.nii'\n",
    ",'smwp10100_T1.nii'\n",
    ",'smwp10084_T1.nii'\n",
    ",'smwp10036_T1.nii'\n",
    ",'smwp10032_T1.nii'\n",
    ",'smwp10150_T1.nii'\n",
    ",'smwp10160_T1.nii'\n",
    ",'smwp10090_T1.nii'\n",
    ",'smwp10015_T1.nii'\n",
    ",'smwp10106_T1.nii'\n",
    ",'smwp10055_T1.nii'\n",
    ",'smwp10164_T1.nii'\n",
    ",'smwp10038_T1.nii'\n",
    ",'smwp10120_T1.nii'\n",
    ",'smwp10053_T1.nii'\n",
    ",'smwp10020_T1.nii'\n",
    ",'smwp10133_T1.nii'\n",
    ",'smwp10085_T1.nii'\n",
    ",'smwp10140_T1.nii'\n",
    ",'smwp10080_T1.nii'\n",
    ",'smwp10056_T1.nii'\n",
    ",'smwp10158_T1.nii'\n",
    ",'smwp10135_T1.nii'\n",
    ",'smwp10107_T1.nii'\n",
    ",'smwp10091_T1.nii'\n",
    ",'smwp10128_T1.nii'\n",
    ",'smwp10063_T1.nii'\n",
    ",'smwp10137_T1.nii'\n",
    ",'smwp10068_T1.nii'\n",
    ",'smwp10105_T1.nii'\n",
    ",'smwp10052_T1.nii'\n",
    ",'smwp10078_T1.nii'\n",
    ",'smwp10149_T1.nii'\n",
    ",'smwp10047_T1.nii'\n",
    ",'smwp10121_T1.nii'\n",
    "]\n",
    "\n",
    "# Map filenames to their corresponding indices in the full dataset\n",
    "filename_to_index = {fname: i for i, fname in enumerate(filenames)}\n",
    "\n",
    "# Extract indices for training and validation sets\n",
    "train_indices = [filename_to_index[fname] for fname in train_filenames]\n",
    "val_indices = [filename_to_index[fname] for fname in val_filenames]\n",
    "\n",
    "# latents = np.array(latents)  # Convert to NumPy array\n",
    "latents_array = np.array(latents).reshape(len(latents), -1)\n",
    "\n",
    "PT_500_np = np.array(PT_500)  # Convert to NumPy array\n",
    "\n",
    "# Use indices to extract the data for training and validation sets\n",
    "X_train = latents_array[train_indices]\n",
    "y_train = PT_500_np[train_indices]\n",
    "X_test = latents_array[val_indices]\n",
    "y_test = PT_500_np[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "43ab0a89-92f9-47a0-9d33-d2745e0b8046",
   "metadata": {},
   "outputs": [],
   "source": [
    "PT_4000_np = np.array(PT_4000)  # Convert to NumPy array\n",
    "\n",
    "# Use indices to extract the data for training and validation sets\n",
    "X_train_4000 = latents_array[train_indices]\n",
    "y_train_4000 = PT_4000_np[train_indices]\n",
    "X_test_4000 = latents_array[val_indices]\n",
    "y_test_4000 = PT_4000_np[val_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e627abcd-a7a8-44d9-80f9-a303666f01b6",
   "metadata": {},
   "source": [
    "# Random Forest on Latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd3380f8-835c-4dfa-b772-0bfed278231b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.23 10.9  13.79 25.67 17.55 12.08  6.51 12.08  7.7  13.77 11.02 22.2\n",
      " 12.61 18.28 13.13 14.57 10.65  9.11 13.04 18.97  7.64 14.51 12.19 16.06\n",
      " 18.05  8.87 12.65 12.03 13.44  7.27 10.79 12.76  7.82 11.48 10.73 10.74\n",
      " 14.48 17.19 14.63 12.35  8.75 14.08 12.04 11.25 15.19 10.75  8.41  8.22\n",
      " 11.36  6.73 11.43 10.87]\n",
      "Random Forest MSE: 87.91819230769232\n",
      "[32.29 19.43 45.88 48.18 36.04 32.2  31.79 23.36 19.58 42.48 23.73 44.85\n",
      " 29.18 43.46 27.15 47.4  24.92 31.05 34.28 50.47 25.24 44.68 27.27 35.67\n",
      " 32.3  34.07 28.85 34.84 38.17 24.41 33.62 41.76 25.45 46.13 42.2  32.01\n",
      " 37.77 42.28 25.22 45.89 26.33 34.67 42.06 47.35 31.03 24.58 23.45 25.49\n",
      " 25.3  27.07 31.43 25.39]\n",
      "Random Forest MSE: 484.9615153846154\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "latents_array = np.array(latents).reshape(len(latents), -1)\n",
    "PT_500_np = np.array(PT_500) \n",
    "PT_4000_np = np.array(PT_4000) \n",
    "\n",
    "\n",
    "# Splitting the latent representations and their corresponding numerical values into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(latents_array, PT_500_np, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train RandomForestRegressor\n",
    "regressor = RandomForestRegressor(n_estimators=100)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "predictions = regressor.predict(X_test)\n",
    "print(predictions)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Random Forest MSE: {mse}')\n",
    "\n",
    "# Splitting the latent representations and their corresponding numerical values into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(latents_array, PT_4000_np, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train RandomForestRegressor\n",
    "regressor = RandomForestRegressor(n_estimators=100)\n",
    "regressor.fit(X_train_4000, y_train_4000)\n",
    "\n",
    "# Evaluation\n",
    "predictions = regressor.predict(X_test_4000)\n",
    "print(predictions)\n",
    "mse = mean_squared_error(y_test_4000, predictions)\n",
    "print(f'Random Forest MSE: {mse}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95740049-d541-4645-9750-b8ff530a7158",
   "metadata": {},
   "source": [
    "# XGB on Latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e81db48-c26f-445b-b8e6-ffed540b0343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth [ 5  5  5 10 13 23 10 -5  8  3 15  5  0 13  8  0 25 10 10 18  5  8 10  0\n",
      " 25 13  8  3 18 20 10 23  0 13  8  5 15  5  3 13  9  0 38 15 20 20  0 -3\n",
      "  3 20  8  8]\n",
      "[12.171845   8.574337  22.992125  28.139708  19.173525  14.494497\n",
      "  8.512958   9.135817   5.9693723 11.157347  12.2764015 19.729847\n",
      " 10.261713  12.020353  10.392263  14.680391   9.087205  13.104547\n",
      " 11.425393  24.539078  13.62485    9.414397   4.1595197  9.837781\n",
      " 13.120654   9.0325365 14.6752205  4.5429487 11.484718   7.8316965\n",
      " 12.402602  10.440407   7.7590656 16.318987   8.429059   9.79118\n",
      "  6.3139925 12.814491  13.59283   13.953571   3.664246  14.316917\n",
      " 14.1813965  9.6992855  6.4567213 15.177594   7.280269   9.985841\n",
      "  2.3351972  9.1552    10.625794   9.174586 ]\n",
      "XGBoost MSE for PT_500: 85.62056206042345\n",
      "ground truth [78  5 45 20 13 48 18 -3  5  3 55 10  8 25 23 28 35 23  3 45 15  5 48  5\n",
      " 48 53 10 23 53 58 20 35 -3 63  5 20 18 15 30 30 55 45 40 45 18 60 -3 13\n",
      " 45 28 10 43]\n",
      "[23.19543   6.839634 38.217842 57.720615 28.418728 41.321747 28.809952\n",
      " 30.33156  33.946156 47.870113 26.586912 51.342556 26.273943 34.216736\n",
      " 30.288328 38.120167 22.918053  9.766158 41.49415  57.845547 23.945328\n",
      " 44.61806  24.9872   31.32042  44.264904 29.723892 31.147385 22.13361\n",
      " 29.16369  27.247362 38.90995  45.838406 28.302029 40.671814 46.1257\n",
      " 23.134043 27.142254 34.787666 30.92004  48.971153  5.768208 30.565023\n",
      " 56.47532  40.38859  28.077106 19.387018 22.690657 17.287603 32.13194\n",
      " 19.353588 17.268597 39.08215 ]\n",
      "XGBoost MSE for PT_4000: 566.1294492756377\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "latents_array = np.array(latents).reshape(len(latents), -1)  # Your flattened latent vectors\n",
    "PT_500_np = np.array(PT_500)  # PT_500 numerical values\n",
    "PT_4000_np = np.array(PT_4000)  # PT_4000 numerical values\n",
    "\n",
    "# Predicting PT_500 values using XGBoost\n",
    "# X_train, X_test, y_train, y_test = train_test_split(latents_array, PT_500_np, test_size=0.3, random_state=42)\n",
    "\n",
    "xgb_regressor = xgb.XGBRegressor(n_estimators=100, objective='reg:squarederror')\n",
    "xgb_regressor.fit(X_train, y_train)\n",
    "\n",
    "predictions = xgb_regressor.predict(X_test)\n",
    "print(\"ground truth\",y_test)\n",
    "print(predictions)\n",
    "\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'XGBoost MSE for PT_500: {mse}')\n",
    "\n",
    "# Predicting PT_4000 values using XGBoost\n",
    "# X_train, X_test, y_train, y_test = train_test_split(latents_array, PT_4000_np, test_size=0.3, random_state=42)\n",
    "\n",
    "xgb_regressor = xgb.XGBRegressor(n_estimators=100, objective='reg:squarederror')\n",
    "xgb_regressor.fit(X_train_4000, y_train_4000)\n",
    "\n",
    "predictions = xgb_regressor.predict(X_test_4000)\n",
    "print(\"ground truth\",y_test_4000)\n",
    "print(predictions)\n",
    "mse = mean_squared_error(y_test_4000, predictions)\n",
    "print(f'XGBoost MSE for PT_4000: {mse}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc203fa3-3143-4f88-a666-4b38a00fccd7",
   "metadata": {},
   "source": [
    "# MNN Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "433c4143-6812-4d88-9cd9-ddf002e4ddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, dropout_rate=0.5):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)  # Dropout layer after first ReLU\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)  # Dropout layer after second ReLU\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x, feature_extract=False):\n",
    "        x = self.dropout1(self.relu1(self.fc1(x)))\n",
    "        x = self.dropout2(self.relu2(self.fc2(x)))\n",
    "        if feature_extract:\n",
    "            # Return features just before the last dropout for feature extraction\n",
    "            return x\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def extract_features(self, dataloader):\n",
    "        self.eval()  # Set model to evaluation mode\n",
    "        features = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, _ in dataloader:\n",
    "                outputs = self(inputs, feature_extract=True)\n",
    "                features.append(outputs)\n",
    "        return torch.cat(features, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfadda8-e28a-4dba-b4a9-08dca17d7952",
   "metadata": {},
   "source": [
    "# MNN (2 hidden layers) on Latents on PT500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae7bff25-aec1-4446-b8be-16b91bded026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 181.8996\n",
      "Epoch [11/100], Loss: 153.0902\n",
      "Epoch [21/100], Loss: 182.1541\n",
      "Epoch [31/100], Loss: 88.4302\n",
      "Epoch [41/100], Loss: 113.6936\n",
      "Epoch [51/100], Loss: 36.3566\n",
      "Epoch [61/100], Loss: 156.6059\n",
      "Epoch [71/100], Loss: 73.9263\n",
      "Epoch [81/100], Loss: 88.2122\n",
      "Epoch [91/100], Loss: 39.7692\n",
      "Test RMSE: 9.7935\n",
      "Ground Truth: 5.0000, Prediction: 11.2609\n",
      "Ground Truth: 5.0000, Prediction: 0.9547\n",
      "Ground Truth: 5.0000, Prediction: 14.9995\n",
      "Ground Truth: 10.0000, Prediction: 28.4357\n",
      "Ground Truth: 13.0000, Prediction: 17.2401\n",
      "Ground Truth: 23.0000, Prediction: 15.0993\n",
      "Ground Truth: 10.0000, Prediction: 7.4128\n",
      "Ground Truth: -5.0000, Prediction: 6.6912\n",
      "Ground Truth: 8.0000, Prediction: 6.9261\n",
      "Ground Truth: 3.0000, Prediction: 15.6006\n",
      "Ground Truth: 15.0000, Prediction: 12.0102\n",
      "Ground Truth: 5.0000, Prediction: 26.5010\n",
      "Ground Truth: 0.0000, Prediction: 14.7120\n",
      "Ground Truth: 13.0000, Prediction: 14.1091\n",
      "Ground Truth: 8.0000, Prediction: 13.1790\n",
      "Ground Truth: 0.0000, Prediction: 19.4183\n",
      "Ground Truth: 25.0000, Prediction: 13.7972\n",
      "Ground Truth: 10.0000, Prediction: 8.5779\n",
      "Ground Truth: 10.0000, Prediction: 10.4475\n",
      "Ground Truth: 18.0000, Prediction: 19.2836\n",
      "Ground Truth: 5.0000, Prediction: 10.0785\n",
      "Ground Truth: 8.0000, Prediction: 11.7235\n",
      "Ground Truth: 10.0000, Prediction: 9.4774\n",
      "Ground Truth: 0.0000, Prediction: 23.0031\n",
      "Ground Truth: 25.0000, Prediction: 14.2060\n",
      "Ground Truth: 13.0000, Prediction: 13.2381\n",
      "Ground Truth: 8.0000, Prediction: 7.2918\n",
      "Ground Truth: 3.0000, Prediction: 12.9941\n",
      "Ground Truth: 18.0000, Prediction: 13.9372\n",
      "Ground Truth: 20.0000, Prediction: 9.4071\n",
      "Ground Truth: 10.0000, Prediction: 14.1932\n",
      "Ground Truth: 23.0000, Prediction: 14.9097\n",
      "Ground Truth: 0.0000, Prediction: 12.0859\n",
      "Ground Truth: 13.0000, Prediction: 12.6330\n",
      "Ground Truth: 8.0000, Prediction: 13.7752\n",
      "Ground Truth: 5.0000, Prediction: 11.1342\n",
      "Ground Truth: 15.0000, Prediction: 11.1017\n",
      "Ground Truth: 5.0000, Prediction: 14.2622\n",
      "Ground Truth: 3.0000, Prediction: 13.7223\n",
      "Ground Truth: 13.0000, Prediction: 15.5634\n",
      "Ground Truth: 9.0000, Prediction: 7.1932\n",
      "Ground Truth: 0.0000, Prediction: 12.6631\n",
      "Ground Truth: 38.0000, Prediction: 11.8021\n",
      "Ground Truth: 15.0000, Prediction: 17.9769\n",
      "Ground Truth: 20.0000, Prediction: 11.7991\n",
      "Ground Truth: 20.0000, Prediction: 17.1436\n",
      "Ground Truth: 0.0000, Prediction: 11.9426\n",
      "Ground Truth: -3.0000, Prediction: 9.1402\n",
      "Ground Truth: 3.0000, Prediction: 14.5694\n",
      "Ground Truth: 20.0000, Prediction: 11.7127\n",
      "Ground Truth: 8.0000, Prediction: 8.4827\n",
      "Ground Truth: 8.0000, Prediction: 13.7315\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(latents_array, PT_500_np, test_size=0.3, random_state=42)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # Use the same scaler to transform the test set\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # Add extra dimension for targets\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)  # Shuffle=False for testing\n",
    "\n",
    "    \n",
    "# Instantiate the model\n",
    "dropout_rate = 0.5  # Dropout rate can be adjusted as needed\n",
    "model = SimpleNN(input_size=X_train_tensor.shape[1], hidden_size1=64, hidden_size2=32, output_size=1, dropout_rate=dropout_rate)\n",
    "model.train()\n",
    "\n",
    "l2_lambda = 0.01  # L2 regularization strength\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=l2_lambda)  # L2 regularization is added via weight_decay\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_features = model.extract_features(train_dataloader)\n",
    "test_features = model.extract_features(test_dataloader)\n",
    "\n",
    "X_train_features = train_features.numpy()\n",
    "X_test_features = test_features.numpy()\n",
    "\n",
    "# Use MSE Loss and the Adam optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    for inputs, targets in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "# Testing phase\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "test_losses = []\n",
    "all_targets = []  # List to store all the ground truth values\n",
    "all_predictions = []  # List to store all the model's predictions\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_dataloader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_losses.append(loss.item())\n",
    "        \n",
    "        all_targets.extend(targets.view(-1).tolist())  # Flatten and convert to list\n",
    "        all_predictions.extend(outputs.view(-1).tolist())  # Flatten and convert to list\n",
    "\n",
    "# Calculate average loss across all test batches\n",
    "average_test_loss = sum(test_losses) / len(test_losses)\n",
    "test_rmse = math.sqrt(average_test_loss)\n",
    "print(f'Test RMSE: {test_rmse:.4f}')\n",
    "\n",
    "# Display ground truth vs. predictions\n",
    "for i in range(len(all_targets)):\n",
    "    print(f'Ground Truth: {all_targets[i]:.4f}, Prediction: {all_predictions[i]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b135b54-5ef3-4cf8-bbef-3404111baccb",
   "metadata": {},
   "source": [
    "# Ensemble of MNN and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70301b41-4fb6-4274-9727-4c5fe99c21b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 10.6307\n",
      "Ground Truth: 5.0000, Prediction: 24.7490\n",
      "Ground Truth: 5.0000, Prediction: 11.6028\n",
      "Ground Truth: 5.0000, Prediction: 15.1078\n",
      "Ground Truth: 10.0000, Prediction: 11.9791\n",
      "Ground Truth: 13.0000, Prediction: 11.6285\n",
      "Ground Truth: 23.0000, Prediction: 11.1188\n",
      "Ground Truth: 10.0000, Prediction: 16.8362\n",
      "Ground Truth: -5.0000, Prediction: 17.9167\n",
      "Ground Truth: 8.0000, Prediction: 15.7339\n",
      "Ground Truth: 3.0000, Prediction: 18.1512\n",
      "Ground Truth: 15.0000, Prediction: 14.6219\n",
      "Ground Truth: 5.0000, Prediction: 11.8281\n",
      "Ground Truth: 0.0000, Prediction: 10.8780\n",
      "Ground Truth: 13.0000, Prediction: 11.8248\n",
      "Ground Truth: 8.0000, Prediction: 7.5692\n",
      "Ground Truth: 0.0000, Prediction: 14.9015\n",
      "Ground Truth: 25.0000, Prediction: 12.0865\n",
      "Ground Truth: 10.0000, Prediction: 9.9673\n",
      "Ground Truth: 10.0000, Prediction: 9.0826\n",
      "Ground Truth: 18.0000, Prediction: 15.6674\n",
      "Ground Truth: 5.0000, Prediction: 15.1696\n",
      "Ground Truth: 8.0000, Prediction: 14.4954\n",
      "Ground Truth: 10.0000, Prediction: 12.7026\n",
      "Ground Truth: 0.0000, Prediction: 10.5323\n",
      "Ground Truth: 25.0000, Prediction: 14.4911\n",
      "Ground Truth: 13.0000, Prediction: 11.7720\n",
      "Ground Truth: 8.0000, Prediction: 12.0462\n",
      "Ground Truth: 3.0000, Prediction: 18.1015\n",
      "Ground Truth: 18.0000, Prediction: 11.4272\n",
      "Ground Truth: 20.0000, Prediction: 19.8533\n",
      "Ground Truth: 10.0000, Prediction: 21.1422\n",
      "Ground Truth: 23.0000, Prediction: 11.2685\n",
      "Ground Truth: 0.0000, Prediction: 15.0653\n",
      "Ground Truth: 13.0000, Prediction: 16.6326\n",
      "Ground Truth: 8.0000, Prediction: 12.7765\n",
      "Ground Truth: 5.0000, Prediction: 7.7833\n",
      "Ground Truth: 15.0000, Prediction: 19.0297\n",
      "Ground Truth: 5.0000, Prediction: 14.9505\n",
      "Ground Truth: 3.0000, Prediction: 12.9219\n",
      "Ground Truth: 13.0000, Prediction: 14.0965\n",
      "Ground Truth: 9.0000, Prediction: 10.8005\n",
      "Ground Truth: 0.0000, Prediction: 17.1086\n",
      "Ground Truth: 38.0000, Prediction: 8.4632\n",
      "Ground Truth: 15.0000, Prediction: 37.0319\n",
      "Ground Truth: 20.0000, Prediction: 17.0536\n",
      "Ground Truth: 20.0000, Prediction: 14.3164\n",
      "Ground Truth: 0.0000, Prediction: 11.3939\n",
      "Ground Truth: -3.0000, Prediction: 13.2446\n",
      "Ground Truth: 3.0000, Prediction: 14.9274\n",
      "Ground Truth: 20.0000, Prediction: 11.4265\n",
      "Ground Truth: 8.0000, Prediction: 15.4555\n",
      "Ground Truth: 8.0000, Prediction: 7.6344\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Instantiate the model\n",
    "model = XGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=100, subsample=0.8, colsample_bytree=0.8, objective='reg:squarederror')\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train_features, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test_features)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = math.sqrt(mean_squared_error(y_test, predictions))\n",
    "print(f\"Test RMSE: {rmse:.4f}\")\n",
    "for i, (gt, pred) in enumerate(zip(y_test, predictions)):\n",
    "    print(f'Ground Truth: {gt:.4f}, Prediction: {pred:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222f6d22-2797-4c24-9326-55024b56fbd9",
   "metadata": {},
   "source": [
    "# MNN (2 hidden layers) on Latents on PT4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2510086e-1f60-459c-a087-d0628d71a929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1475.6288\n",
      "Epoch [11/100], Loss: 1571.0656\n",
      "Epoch [21/100], Loss: 617.1367\n",
      "Epoch [31/100], Loss: 622.1299\n",
      "Epoch [41/100], Loss: 604.7731\n",
      "Epoch [51/100], Loss: 361.0830\n",
      "Epoch [61/100], Loss: 361.5636\n",
      "Epoch [71/100], Loss: 373.6135\n",
      "Epoch [81/100], Loss: 221.1975\n",
      "Epoch [91/100], Loss: 416.6325\n",
      "Test RMSE: 25.8575\n",
      "Ground Truth: 78.0000, Prediction: 34.5438\n",
      "Ground Truth: 5.0000, Prediction: -3.8884\n",
      "Ground Truth: 45.0000, Prediction: 39.1312\n",
      "Ground Truth: 20.0000, Prediction: 67.8551\n",
      "Ground Truth: 13.0000, Prediction: 25.8748\n",
      "Ground Truth: 48.0000, Prediction: 35.1492\n",
      "Ground Truth: 18.0000, Prediction: 26.2763\n",
      "Ground Truth: -3.0000, Prediction: 19.7647\n",
      "Ground Truth: 5.0000, Prediction: 17.9533\n",
      "Ground Truth: 3.0000, Prediction: 41.5584\n",
      "Ground Truth: 55.0000, Prediction: 21.1442\n",
      "Ground Truth: 10.0000, Prediction: 80.1423\n",
      "Ground Truth: 8.0000, Prediction: 36.5891\n",
      "Ground Truth: 25.0000, Prediction: 37.9988\n",
      "Ground Truth: 23.0000, Prediction: 27.1078\n",
      "Ground Truth: 28.0000, Prediction: 57.2402\n",
      "Ground Truth: 35.0000, Prediction: 30.8920\n",
      "Ground Truth: 23.0000, Prediction: 24.4812\n",
      "Ground Truth: 3.0000, Prediction: 33.3285\n",
      "Ground Truth: 45.0000, Prediction: 37.5668\n",
      "Ground Truth: 15.0000, Prediction: 18.4171\n",
      "Ground Truth: 5.0000, Prediction: 36.0472\n",
      "Ground Truth: 48.0000, Prediction: 29.5863\n",
      "Ground Truth: 5.0000, Prediction: 58.8057\n",
      "Ground Truth: 48.0000, Prediction: 36.0863\n",
      "Ground Truth: 53.0000, Prediction: 33.2092\n",
      "Ground Truth: 10.0000, Prediction: 26.2230\n",
      "Ground Truth: 23.0000, Prediction: 23.1086\n",
      "Ground Truth: 53.0000, Prediction: 43.2159\n",
      "Ground Truth: 58.0000, Prediction: 26.1786\n",
      "Ground Truth: 20.0000, Prediction: 45.1573\n",
      "Ground Truth: 35.0000, Prediction: 52.1874\n",
      "Ground Truth: -3.0000, Prediction: 36.7074\n",
      "Ground Truth: 63.0000, Prediction: 38.2577\n",
      "Ground Truth: 5.0000, Prediction: 44.8585\n",
      "Ground Truth: 20.0000, Prediction: 34.9548\n",
      "Ground Truth: 18.0000, Prediction: 35.1193\n",
      "Ground Truth: 15.0000, Prediction: 45.0475\n",
      "Ground Truth: 30.0000, Prediction: 31.2415\n",
      "Ground Truth: 30.0000, Prediction: 55.7737\n",
      "Ground Truth: 55.0000, Prediction: 11.8675\n",
      "Ground Truth: 45.0000, Prediction: 26.8703\n",
      "Ground Truth: 40.0000, Prediction: 37.7513\n",
      "Ground Truth: 45.0000, Prediction: 58.1469\n",
      "Ground Truth: 18.0000, Prediction: 32.9114\n",
      "Ground Truth: 60.0000, Prediction: 19.4687\n",
      "Ground Truth: -3.0000, Prediction: 35.7275\n",
      "Ground Truth: 13.0000, Prediction: 26.1698\n",
      "Ground Truth: 45.0000, Prediction: 25.7619\n",
      "Ground Truth: 28.0000, Prediction: 18.1059\n",
      "Ground Truth: 10.0000, Prediction: 8.4202\n",
      "Ground Truth: 43.0000, Prediction: 27.5269\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = X_train_4000, X_test_4000, y_train_4000, y_test_4000\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(latents_array, PT_4000_np, test_size=0.3, random_state=42)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # Use the same scaler to transform the test set\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # Add extra dimension for targets\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)  # Shuffle=False for testing\n",
    "    \n",
    "# Instantiate the model\n",
    "dropout_rate = 0.5  # Dropout rate can be adjusted as needed\n",
    "model = SimpleNN(input_size=X_train_tensor.shape[1], hidden_size1=64, hidden_size2=32, output_size=1, dropout_rate=dropout_rate)\n",
    "model.train()\n",
    "\n",
    "l2_lambda = 0.01  # L2 regularization strength\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=l2_lambda)  # L2 regularization is added via weight_decay\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_features = model.extract_features(train_dataloader)\n",
    "test_features = model.extract_features(test_dataloader)\n",
    "\n",
    "X_train_features = train_features.numpy()\n",
    "X_test_features = test_features.numpy()\n",
    "\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    for inputs, targets in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "# Testing phase\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "test_losses = []\n",
    "all_targets = []  # List to store all the ground truth values\n",
    "all_predictions = []  # List to store all the model's predictions\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_dataloader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_losses.append(loss.item())\n",
    "        \n",
    "        all_targets.extend(targets.view(-1).tolist())  # Flatten and convert to list\n",
    "        all_predictions.extend(outputs.view(-1).tolist())  # Flatten and convert to list\n",
    "\n",
    "# Calculate average loss across all test batches\n",
    "average_test_loss = sum(test_losses) / len(test_losses)\n",
    "test_rmse = math.sqrt(average_test_loss)\n",
    "print(f'Test RMSE: {test_rmse:.4f}')\n",
    "\n",
    "# Display ground truth vs. predictions\n",
    "for i in range(len(all_targets)):\n",
    "    print(f'Ground Truth: {all_targets[i]:.4f}, Prediction: {all_predictions[i]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0565bc8-adc9-4313-80cf-740e373d77cc",
   "metadata": {},
   "source": [
    "# Ensemble of MNN and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43ff8f25-2624-4045-9086-61b4e90d6b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 23.3541\n",
      "Ground Truth: 78.0000, Prediction: 26.1377\n",
      "Ground Truth: 5.0000, Prediction: 38.9419\n",
      "Ground Truth: 45.0000, Prediction: 47.0229\n",
      "Ground Truth: 20.0000, Prediction: 46.4734\n",
      "Ground Truth: 13.0000, Prediction: 46.2009\n",
      "Ground Truth: 48.0000, Prediction: 22.5880\n",
      "Ground Truth: 18.0000, Prediction: 29.0854\n",
      "Ground Truth: -3.0000, Prediction: 30.8086\n",
      "Ground Truth: 5.0000, Prediction: 29.0428\n",
      "Ground Truth: 3.0000, Prediction: 21.6108\n",
      "Ground Truth: 55.0000, Prediction: 33.7466\n",
      "Ground Truth: 10.0000, Prediction: 51.8923\n",
      "Ground Truth: 8.0000, Prediction: 18.4383\n",
      "Ground Truth: 25.0000, Prediction: 34.4669\n",
      "Ground Truth: 23.0000, Prediction: 22.5806\n",
      "Ground Truth: 28.0000, Prediction: 36.9611\n",
      "Ground Truth: 35.0000, Prediction: 42.5925\n",
      "Ground Truth: 23.0000, Prediction: 30.4321\n",
      "Ground Truth: 3.0000, Prediction: 43.2701\n",
      "Ground Truth: 45.0000, Prediction: 42.1412\n",
      "Ground Truth: 15.0000, Prediction: 19.4155\n",
      "Ground Truth: 5.0000, Prediction: 42.4016\n",
      "Ground Truth: 48.0000, Prediction: 48.8452\n",
      "Ground Truth: 5.0000, Prediction: 37.6864\n",
      "Ground Truth: 48.0000, Prediction: 48.2799\n",
      "Ground Truth: 53.0000, Prediction: 27.4319\n",
      "Ground Truth: 10.0000, Prediction: 21.5851\n",
      "Ground Truth: 23.0000, Prediction: 32.0635\n",
      "Ground Truth: 53.0000, Prediction: 27.1715\n",
      "Ground Truth: 58.0000, Prediction: 38.1624\n",
      "Ground Truth: 20.0000, Prediction: 17.4361\n",
      "Ground Truth: 35.0000, Prediction: 73.7786\n",
      "Ground Truth: -3.0000, Prediction: 36.2833\n",
      "Ground Truth: 63.0000, Prediction: 33.1497\n",
      "Ground Truth: 5.0000, Prediction: 38.1682\n",
      "Ground Truth: 20.0000, Prediction: 22.3915\n",
      "Ground Truth: 18.0000, Prediction: 34.1147\n",
      "Ground Truth: 15.0000, Prediction: 53.7163\n",
      "Ground Truth: 30.0000, Prediction: 35.3140\n",
      "Ground Truth: 30.0000, Prediction: 35.9215\n",
      "Ground Truth: 55.0000, Prediction: 30.1391\n",
      "Ground Truth: 45.0000, Prediction: 28.1020\n",
      "Ground Truth: 40.0000, Prediction: 33.5508\n",
      "Ground Truth: 45.0000, Prediction: 36.1047\n",
      "Ground Truth: 18.0000, Prediction: 45.9668\n",
      "Ground Truth: 60.0000, Prediction: 27.3911\n",
      "Ground Truth: -3.0000, Prediction: 25.4438\n",
      "Ground Truth: 13.0000, Prediction: 18.0792\n",
      "Ground Truth: 45.0000, Prediction: 35.7666\n",
      "Ground Truth: 28.0000, Prediction: 28.4703\n",
      "Ground Truth: 10.0000, Prediction: 32.9150\n",
      "Ground Truth: 43.0000, Prediction: 49.4927\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Instantiate the model\n",
    "model = XGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=100, subsample=0.8, colsample_bytree=0.8, objective='reg:squarederror')\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train_features, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test_features)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = math.sqrt(mean_squared_error(y_test, predictions))\n",
    "print(f\"Test RMSE: {rmse:.4f}\")\n",
    "for i, (gt, pred) in enumerate(zip(y_test, predictions)):\n",
    "    print(f'Ground Truth: {gt:.4f}, Prediction: {pred:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504f221d-1daf-4af1-84be-48a4cf5dcb09",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab2070f5-0ad3-4814-8923-def1fa06d4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, dropout_rate=0.5):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)  # Dropout layer after first ReLU\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)  # Dropout layer after second ReLU\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.model = model_VAE()\n",
    "\n",
    "    def forward(self, img, feature_extract=False):\n",
    "        \n",
    "        x = self.model(img)\n",
    "        x = self.dropout1(self.relu1(self.fc1(x)))\n",
    "        x = self.dropout2(self.relu2(self.fc2(x)))\n",
    "        if feature_extract:\n",
    "            # Return features just before the last dropout for feature extraction\n",
    "            return x\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def extract_features(self, dataloader):\n",
    "        self.eval()  # Set model to evaluation mode\n",
    "        features = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, _ in dataloader:\n",
    "                outputs = self(inputs, feature_extract=True)\n",
    "                features.append(outputs)\n",
    "        return torch.cat(features, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d7e588-d5ed-47a2-b7d2-9504832f1138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training loop\n",
    "# epochs = 100\n",
    "# for epoch in range(epochs):\n",
    "#     for inputs, targets in train_dataloader:\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, targets)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "    \n",
    "#     if epoch % 10 == 0:\n",
    "#         print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "# # Testing phase\n",
    "# model.eval()  # Set the model to evaluation mode\n",
    "# test_losses = []\n",
    "# all_targets = []  # List to store all the ground truth values\n",
    "# all_predictions = []  # List to store all the model's predictions\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for inputs, targets in test_dataloader:\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, targets)\n",
    "#         test_losses.append(loss.item())\n",
    "        \n",
    "#         all_targets.extend(targets.view(-1).tolist())  # Flatten and convert to list\n",
    "#         all_predictions.extend(outputs.view(-1).tolist())  # Flatten and convert to list\n",
    "\n",
    "# # Calculate average loss across all test batches\n",
    "# average_test_loss = sum(test_losses) / len(test_losses)\n",
    "# test_rmse = math.sqrt(average_test_loss)\n",
    "# print(f'Test RMSE: {test_rmse:.4f}')\n",
    "\n",
    "# # Display ground truth vs. predictions\n",
    "# for i in range(len(all_targets)):\n",
    "#     print(f'Ground Truth: {all_targets[i]:.4f}, Prediction: {all_predictions[i]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b132c9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13010895",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-a100",
   "language": "python",
   "name": "pytorch-a100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
